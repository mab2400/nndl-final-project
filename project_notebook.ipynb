{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "erhBxMGhid2n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data labels (subclasses and superclasses)"
      ],
      "metadata": {
        "id": "Gq6PVBDLHf0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load mappings\n",
        "superclass_df = pd.read_csv('/content/superclass_mapping.csv')\n",
        "subclass_df = pd.read_csv('/content/subclass_mapping.csv')\n",
        "num_superclasses = len(superclass_df)\n",
        "num_subclasses = len(subclass_df)\n",
        "\n",
        "get_superclass = dict(zip(superclass_df['index'], superclass_df['class']))\n",
        "get_subclass = dict(zip(subclass_df['index'], subclass_df['class']))"
      ],
      "metadata": {
        "id": "QOiagOmPHe9w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Training and Validation Datasets"
      ],
      "metadata": {
        "id": "6vzBXtuocENr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "!unzip '/content/train_images.zip'\n",
        "!unzip '/content/test_images.zip'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o0IK2ongdtKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load annotation CSV\n",
        "ann_df = pd.read_csv('/content/train_data.csv')\n",
        "\n",
        "# Images path\n",
        "img_dir = '/content/train_images/'\n",
        "\n",
        "# Split into training (90%) and validation (10%) sets\n",
        "train_df, val_df = train_test_split(ann_df, test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "IMAGE_SHAPE = [380, 380] # Because of EfficientNetB4\n",
        "\n",
        "# Pre-Processing WITHOUT AUGMENTATION\n",
        "def process_image_no_aug(img_path, super_label, sub_label):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "\n",
        "    # Create one-hot encodings\n",
        "    super_label = tf.one_hot(super_label, depth=num_superclasses)\n",
        "    sub_label = tf.one_hot(sub_label, depth=num_subclasses)\n",
        "\n",
        "    return img, {'super_output': super_label, 'sub_output': sub_label}\n",
        "\n",
        "# Pre-Processing WITH AUGMENTATION\n",
        "def process_image_with_aug(img_path, super_label, sub_label):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "\n",
        "    # Data augmentation\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
        "    img = tf.image.random_saturation(img, lower=0.95, upper=1.05)\n",
        "    img = tf.image.random_hue(img, max_delta=0.02)\n",
        "\n",
        "    # Create one-hot encodings\n",
        "    super_label = tf.one_hot(super_label, depth=num_superclasses)\n",
        "    sub_label = tf.one_hot(sub_label, depth=num_subclasses)\n",
        "\n",
        "    return img, {'super_output': super_label, 'sub_output': sub_label}\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def get_image_paths(df, base_dir):\n",
        "  image_paths = []\n",
        "  for filename in df['image']:\n",
        "    image_paths.append(os.path.join(base_dir, filename))\n",
        "  return image_paths\n",
        "\n",
        "# ====== Creating the training dataset =========\n",
        "image_paths = get_image_paths(train_df, '/content/train_images')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_paths, train_df['superclass_index'].values, train_df['subclass_index'].values))\n",
        "# Data augmentation\n",
        "dataset = dataset.map(process_image_with_aug, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ====== Creating the validation dataset =========\n",
        "image_paths = get_image_paths(val_df, '/content/train_images')\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_paths, val_df['superclass_index'].values, val_df['subclass_index'].values))\n",
        "# No data augmentation\n",
        "dataset = dataset.map(process_image_no_aug, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-fSYyPYEXj0C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Test Dataset"
      ],
      "metadata": {
        "id": "6N34DIwxcJNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Creating the test dataset =========\n",
        "test_image_filenames = sorted([f for f in os.listdir('/content/test_images/')])\n",
        "test_image_paths = [os.path.join('/content/test_images/', fname) for fname in test_image_filenames]\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_image_filenames))\n",
        "\n",
        "def process_test_image(img_path, img_name):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "    return img, img_name\n",
        "\n",
        "test_dataset = test_dataset.map(process_test_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "oxGbeZp6b8F2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the Class Imbalance in the Training Dataset"
      ],
      "metadata": {
        "id": "mK7r2AUHruRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code to plot the class imbalances was generated by ChatGPT.\n",
        "\n",
        "# Subclass imbalance\n",
        "\n",
        "# How many times each subclass appears, sorted by subclass index (not by descending count).\n",
        "subclass_counts = train_df['subclass_index'].value_counts().sort_index()\n",
        "\n",
        "print(\"Subclass Distribution:\")\n",
        "print(subclass_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 4))\n",
        "subclass_counts.plot(kind='bar')\n",
        "plt.title('Subclass Distribution')\n",
        "plt.xlabel('Subclass Index')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "#==========\n",
        "\n",
        "# Superclass imbalance\n",
        "\n",
        "# How many times each superclass appears, sorted by superclass index (not by descending count).\n",
        "superclass_counts = train_df['superclass_index'].value_counts().sort_index()\n",
        "\n",
        "print(\"Superclass Distribution:\")\n",
        "print(superclass_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "superclass_counts.plot(kind='bar', color='orange')\n",
        "plt.title('Superclass Distribution')\n",
        "plt.xlabel('Superclass Index')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KRRvvMbfrt20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the Class Imbalance in the Validation Dataset"
      ],
      "metadata": {
        "id": "X92QFC0o7MvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code to plot the class imbalances was generated by ChatGPT.\n",
        "\n",
        "# Subclass imbalance\n",
        "\n",
        "# How many times each subclass appears, sorted by subclass index (not by descending count).\n",
        "subclass_counts = val_df['subclass_index'].value_counts().sort_index()\n",
        "\n",
        "print(\"Subclass Distribution:\")\n",
        "print(subclass_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 4))\n",
        "subclass_counts.plot(kind='bar')\n",
        "plt.title('Subclass Distribution')\n",
        "plt.xlabel('Subclass Index')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "#==========\n",
        "\n",
        "# Superclass imbalance\n",
        "\n",
        "# How many times each superclass appears, sorted by superclass index (not by descending count).\n",
        "superclass_counts = val_df['superclass_index'].value_counts().sort_index()\n",
        "\n",
        "print(\"Superclass Distribution:\")\n",
        "print(superclass_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "superclass_counts.plot(kind='bar', color='orange')\n",
        "plt.title('Superclass Distribution')\n",
        "plt.xlabel('Superclass Index')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BaExwk837QTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Class-Weighted Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "OA27Ojh8fCxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "subclass_labels = ann_df['subclass_index'].values\n",
        "\n",
        "# Compute weights\n",
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(subclass_labels), y=subclass_labels)\n",
        "\n",
        "# Convert to Tensor\n",
        "subclass_class_weights = tf.constant(class_weights_array, dtype=tf.float32)\n",
        "\n",
        "# Dummy weight for the \"novel\" subclass category\n",
        "novel_weight = np.array([1.0], dtype=np.float32)\n",
        "subclass_class_weights = tf.constant(np.concatenate([class_weights_array, novel_weight]), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "mVm403rXeThY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def class_weighted_loss(weights):\n",
        "    # Assign a weight to each class\n",
        "    weights = K.variable(weights)\n",
        "\n",
        "    def loss(y_true, y_pred):\n",
        "        # Avoid log(0) via clipping\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        loss = y_true * K.log(y_pred) * weights\n",
        "        loss = -K.sum(loss, axis=-1)\n",
        "        return loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "subclass_loss_fn = class_weighted_loss(subclass_class_weights)"
      ],
      "metadata": {
        "id": "wbzps7jXfAfO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load EfficientNetB4 as the feature extractor"
      ],
      "metadata": {
        "id": "DKlu-HnLG7DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dual_head_model(input_shape=(380, 380, 3)):\n",
        "    # Load EfficientNetB4 base model\n",
        "    base_model = EfficientNetB4(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = False  # Freeze base model\n",
        "\n",
        "    # Shared feature extraction\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Add 2 classification heads (including the \"novel\" super/sub class in both)\n",
        "    super_output = Dense(num_superclasses, activation='softmax', name='super_output')(x)\n",
        "    sub_output = Dense(num_subclasses, activation='softmax', name='sub_output')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=[super_output, sub_output])\n",
        "    return model\n",
        "\n",
        "model = build_dual_head_model()"
      ],
      "metadata": {
        "id": "WoY-OP4rF1F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with the loss function\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss={\n",
        "        'super_output': 'categorical_crossentropy',  # normal loss\n",
        "        'sub_output': subclass_loss_fn              # weighted loss\n",
        "    },\n",
        "    loss_weights={\n",
        "        'super_output': 1.0,\n",
        "        'sub_output': 1.0\n",
        "    },\n",
        "    metrics={\n",
        "        'super_output': 'accuracy',\n",
        "        'sub_output': 'accuracy'\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "fslVVC3hGPZt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Baseline and get results!"
      ],
      "metadata": {
        "id": "3MlVTRSZUSkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_weighted_model.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "RIypYKM5UUkO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the performance of the Baseline + Weighted Loss Model"
      ],
      "metadata": {
        "id": "9XbIjJtsyyO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below was written with the help of ChatGPT to evaluate the performance of my model.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild and load the model\n",
        "model = build_dual_head_model()\n",
        "model.load_weights('best_weighted_model.h5')\n",
        "\n",
        "# Compile the model using the same settings as training\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss={\n",
        "        'super_output': 'categorical_crossentropy',  # normal loss\n",
        "        'sub_output': subclass_loss_fn              # class-weighted cross-entropy loss\n",
        "    },\n",
        "    metrics={\n",
        "        'super_output': 'accuracy',\n",
        "        'sub_output': 'accuracy'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_results = model.evaluate(val_dataset)\n",
        "print(\"\\nüîπ Validation Results:\")\n",
        "for name, value in zip(model.metrics_names, val_results):\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "# Get the predictions and true labels\n",
        "true_super, pred_super = [], []\n",
        "true_sub, pred_sub = [], []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    images, labels = batch\n",
        "    preds = model.predict(images, verbose=0)\n",
        "\n",
        "    # True labels\n",
        "    true_super += np.argmax(labels['super_output'].numpy(), axis=1).tolist()\n",
        "    true_sub += np.argmax(labels['sub_output'].numpy(), axis=1).tolist()\n",
        "\n",
        "    # Predicted labels\n",
        "    pred_super += np.argmax(preds[0], axis=1).tolist()\n",
        "    pred_sub += np.argmax(preds[1], axis=1).tolist()\n",
        "\n",
        "# Use the predictions and true labels to construct CONFUSION MATRICES\n",
        "super_cm = confusion_matrix(true_super, pred_super)\n",
        "sub_cm = confusion_matrix(true_sub, pred_sub)\n",
        "\n",
        "# Convert to NumPy array if needed\n",
        "sub_cm = np.array(sub_cm)\n",
        "\n",
        "# Convert to DataFrame for easier indexing\n",
        "sub_df = pd.DataFrame(sub_cm)\n",
        "\n",
        "# Zero out the diagonal (correct predictions) so we only see misclassifications\n",
        "sub_df_no_diag = sub_df.copy()\n",
        "np.fill_diagonal(sub_df_no_diag.values, 0)\n",
        "\n",
        "# Find the most confused class pairs (top N)\n",
        "N = 5\n",
        "confused_pairs = sub_df_no_diag.stack().sort_values(ascending=False).head(N)\n",
        "\n",
        "print(\"\\nüîç Top Confused Subclass Pairs:\")\n",
        "for (true_class, pred_class), count in confused_pairs.items():\n",
        "    print(f\"True class {true_class} ‚Üí Predicted class {pred_class}: {count} times\")\n",
        "\n",
        "# Plot Superclass Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(super_cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Superclass Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Subclass Confusion Matrix\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(sub_cm, cmap='Blues')\n",
        "plt.title(\"Subclass Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# F1 scores\n",
        "print(\"\\nüîπ Superclass Classification Report:\")\n",
        "print(classification_report(true_super, pred_super))\n",
        "\n",
        "print(\"\\nüîπ Subclass Classification Report:\")\n",
        "print(classification_report(true_sub, pred_sub))"
      ],
      "metadata": {
        "id": "VLWErRSWdA-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning the Model (only the top few layers starting from 'block6h')"
      ],
      "metadata": {
        "id": "9OUhbbs2svrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_dual_head_model()\n",
        "model.load_weights('best_weighted_model.h5')\n",
        "\n",
        "# Starting freezing from block6h\n",
        "unfreeze = False\n",
        "for layer in model.layers:\n",
        "    if 'block6h' in layer.name:\n",
        "        unfreeze = True\n",
        "    layer.trainable = unfreeze\n",
        "\n",
        "# Compiling the class-weighted-loss model with a SLOWER learning rate (for fine-tuning)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss={\n",
        "        'super_output': 'categorical_crossentropy',  # normal loss\n",
        "        'sub_output': subclass_loss_fn              # weighted loss\n",
        "    },\n",
        "    metrics={\n",
        "        'super_output': 'accuracy',\n",
        "        'sub_output': 'accuracy'\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Cg9sGR9sPsO0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_weighted_finetuned_model_6h.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "5K-X_i7NPsHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the performance of the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "D87C3DYNy41H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below was generated by ChatGPT to help me evaluate the performance of my model.\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild and load the fine-tuned model\n",
        "model = build_dual_head_model()\n",
        "model.load_weights('best_weighted_finetuned_model_6h.h5')\n",
        "\n",
        "# Compile the model using the same settings as training\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss={\n",
        "        'super_output': 'categorical_crossentropy',  # normal loss\n",
        "        'sub_output': subclass_loss_fn              # class-weighted cross-entropy loss\n",
        "    },\n",
        "    metrics={\n",
        "        'super_output': 'accuracy',\n",
        "        'sub_output': 'accuracy'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_results = model.evaluate(val_dataset)\n",
        "print(\"\\nüîπ Validation Results:\")\n",
        "for name, value in zip(model.metrics_names, val_results):\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "# Get the predictions and true labels\n",
        "true_super, pred_super = [], []\n",
        "true_sub, pred_sub = [], []\n",
        "\n",
        "for batch in val_dataset:\n",
        "    images, labels = batch\n",
        "    preds = model.predict(images, verbose=0)\n",
        "\n",
        "    # True labels\n",
        "    true_super += np.argmax(labels['super_output'].numpy(), axis=1).tolist()\n",
        "    true_sub += np.argmax(labels['sub_output'].numpy(), axis=1).tolist()\n",
        "\n",
        "    # Predicted labels\n",
        "    pred_super += np.argmax(preds[0], axis=1).tolist()\n",
        "    pred_sub += np.argmax(preds[1], axis=1).tolist()\n",
        "\n",
        "# Use the predictions and true labels to create CONFUSION MATRICES\n",
        "super_cm = confusion_matrix(true_super, pred_super)\n",
        "sub_cm = confusion_matrix(true_sub, pred_sub)\n",
        "\n",
        "# Plot Superclass Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(super_cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Superclass Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Subclass Confusion Matrix\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(sub_cm, cmap='Blues')\n",
        "plt.title(\"Subclass Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# F1 Scores\n",
        "print(\"\\nüîπ Superclass Classification Report:\")\n",
        "print(classification_report(true_super, pred_super))\n",
        "\n",
        "print(\"\\nüîπ Subclass Classification Report:\")\n",
        "print(classification_report(true_sub, pred_sub))"
      ],
      "metadata": {
        "id": "rmueis8cPsFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the test predictions for the leaderboard"
      ],
      "metadata": {
        "id": "rPyB4OM4y_eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "test_predictions = {\n",
        "    'image': [],\n",
        "    'superclass_index': [],\n",
        "    'subclass_index': []\n",
        "}\n",
        "\n",
        "# Loop through the test dataset and make predictions\n",
        "for batch in test_dataset:  # Each batch contains (images, image_names)\n",
        "    images, image_names = batch\n",
        "    preds = model.predict(images, verbose=0)\n",
        "\n",
        "    super_preds = np.argmax(preds[0], axis=1)\n",
        "    sub_preds = np.argmax(preds[1], axis=1)\n",
        "\n",
        "    for i in range(len(image_names)):\n",
        "        filename = image_names[i].numpy().decode(\"utf-8\")\n",
        "        test_predictions['image'].append(filename)\n",
        "        test_predictions['superclass_index'].append(int(super_preds[i]))\n",
        "        test_predictions['subclass_index'].append(int(sub_preds[i]))\n",
        "\n",
        "# Save predictions to CSV\n",
        "df = pd.DataFrame(test_predictions)\n",
        "df.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Test predictions SAVED\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6o_6qL8zDel",
        "outputId": "d93ebbc1-dcf2-4edd-b938-11c5f8ce1654"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test predictions SAVED\n"
          ]
        }
      ]
    }
  ]
}